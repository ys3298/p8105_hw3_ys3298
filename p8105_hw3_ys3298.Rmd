---
title: "p8105_hw3_ys3298"
author: "Yimeng SHANG"
date: "10/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
```

```{r}
library(p8105.datasets)
library(tidyverse)
library(patchwork)
data("instacart")
data("brfss_smart2010")
accel = read_csv("./accel_data.csv") 
```

```{r include=FALSE}
aisle_num =
  instacart %>%
  group_by(aisle) %>%
  summarise(number = n()) %>%
  count() %>%
  as.integer()
  
aisle_max = 
  instacart %>%
  group_by(aisle) %>%
  summarise(number = n()) %>%
  filter(number == max(number)) %>%
  select(-number) %>%
  as.character()
```
# Problem1
## Short description
**The size and structure of the data:** There are `r nrow(instacart)` observations and `r ncol(instacart)` variables in the data. It's a (`r dim(instacart)`) dataframe. Most of the variable types are integers and some are characters. 

**Some key variables:** Product name; aisle; department; order number and so on. 

**illstrative examples of observations:** For the 1st order, there're 8 different kind of products. Eval set is train. In details, the first product is `r instacart[1,11]`, aisle id is `r instacart[1,12]`, department id is `r instacart[1,13]`, aisle is `r instacart[1,14]`, department is `r instacart[1,15]`. For the second observations, aisle id is `r instacart[2,12]`, department id is `r instacart[2,13]`, aisle is `r instacart[2,14]`, department is `r instacart[2,15]` .

**How many aisles are there, and which aisles are the most items ordered from?** There are `r aisle_num` aisles. The most items ordered from `r aisle_max`.

## Make a plot
```{r echo=FALSE}

  instacart %>%
  group_by(aisle) %>%
  summarise(number = n()) %>%
  filter(number > 10000) %>%
  ggplot(aes(x = reorder(aisle, number), y = number, fill = aisle)) +
  geom_col() + coord_flip() +
  viridis::scale_fill_viridis(option = "viridis" , discrete = TRUE) +
  labs(
    title = "The number of items ordered in each Aisle",
    x = "aisles",
    y = "number"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

From the plot, we can clearly see the rank of number of items ordered in each aisle. The orders from fresh vegetables are the most and  butter is the least.

## Table1: most populat items in each aisles
```{r echo=FALSE}
baking = 
  instacart %>%
  group_by(product_name) %>%
  filter(aisle == "baking ingredients") %>%
  summarise(order_times = n()) %>%
  arrange(desc(order_times)) %>%
  head(n = 3) %>%
  mutate(group = "baking ingredients") %>%
  select(group, everything())

dog = 
  instacart %>%
  group_by(product_name) %>%
  filter(aisle == "dog food care") %>%
  summarise(order_times = n()) %>%
  arrange(desc(order_times)) %>%
  head(n = 3) %>%
  mutate(group = "dog food care") %>%
  select(group, everything())

vege =
  instacart %>%
  group_by(product_name) %>%
  filter(aisle == "packaged vegetables fruits") %>%
  summarise(order_times = n()) %>%
  arrange(desc(order_times)) %>%
  head(n = 3) %>%
  mutate(group = "packaged vegetables fruits") %>%
  select(group, everything())

merge(baking, dog, all = TRUE) %>%
  merge(vege, all = TRUE) %>%
  knitr::kable()
```

From the table, the most popular items from baking ingredients are cane sugar, light brown sugar and pure baking soda. In the group of dog food care, organix chicken&brown rice recipe, small dog biscuits, snack sticks chicken & rice recipe dog treats are the top3 popular items. And in the group of packaged vegetables fruits, organic baby spinach, organic blueberries, organic raspverries sells best.

## Table2: mean hour of a day ordered on each day
```{r echo=FALSE}
instacart %>%
  select(product_name, order_hour_of_day, order_dow) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarise(
    mean_hour = mean(order_hour_of_day)
  ) %>%
  mutate(
    mean_hour = as.integer(mean_hour),
    order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thur", "5" = "Fri", "6" = "Sat")
  ) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>%  
  knitr::kable()
```

From this table, we can clearly see the mean hour of the day at which pink lady apples and coffee ice cream are ordered on each day of the week. In order to see clearly, I round the value to integer.

# Problem2


```{r include=FALSE}
##cleaning
  brfss_OH =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = TRUE )
  )

```

```{r include=FALSE}
location2002 =
  brfss_OH %>%
  filter(year ==  "2002") %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarise(total_location = n()) %>%
  filter(total_location >= 7) %>%
  arrange(total_location)
location2002$locationabbr

location2010 =
  brfss_OH %>%
  filter(year ==  "2010") %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarise(total_location = n()) %>%
  filter(total_location >= 7) %>%
  arrange(total_location)
```
In 2002, `r location2002$locationabbr` were observed at 7 or more locations.

In 2010, `r location2010$locationabbr` were observed at 7 or more locations.

```{r echo=FALSE}
## WHY NA.
Averagedf =
  brfss_OH %>%
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  summarise(average = mean(data_value)) %>%
  select(year, locationabbr, average)

  brfss_OH %>%
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  summarise(average = mean(data_value)) %>%
  select(year, locationabbr, average) %>%
  ggplot(aes(x = year, y = average, color = locationabbr, group = locationabbr)) +
  geom_line() + 
    labs(
      title = "Spaghetti Plot",
      x = "Year",
      y = "Average data value"
    ) +
  viridis::scale_color_viridis(
    name = "States",
    discrete = TRUE) +
    theme(plot.title = element_text(hjust = 0.5))
```

This figure has so many states information, so it's not so clearly to see. We notice that most of the states fluctuate from 2002 to 2010.

```{r echo=FALSE}
a =  brfss_OH %>%
  filter(locationabbr == "NY" & year == "2006" ) %>%
  select(locationdesc, response, data_value) %>%
  group_by(locationdesc) %>%
  ggplot(aes(x = locationdesc, y = data_value, color = response, group = response)) +
  geom_point(size = 4, alpha = .5) + geom_line() +
  labs(
      title = "Response Plot for 2006",
      x = "Year",
      y = "Average data value"
    ) +
  viridis::scale_color_viridis(
    name = "Response",
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))

b =  brfss_OH %>%
  filter(locationabbr == "NY" & year == "2010" ) %>%
  select(locationdesc, response, data_value) %>%
  #group_by(locationdesc) %>%
  ggplot(aes(x = locationdesc, y = data_value, color = response, group = response)) +
  geom_point(size = 4, alpha = .5) + geom_line() +
  labs(
      title = "Response Plot for 2010",
      x = "Year",
      y = "Average data value"
    ) +
  viridis::scale_color_viridis(
    name = "Response",
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))

a + b

```

From the table, we can see the distribution of data value for reponses among locations. "Poor" was the lowest and "Very good" and "Excellent" and relatively high. There is difference in different locations. Form 2006 to 2010, the distribution changed a little bit.

# Problem3
```{r echo=FALSE}
accel = 
  accel %>%
  janitor::clean_names()

weekend =
  accel %>%
  filter(day == "Sunday" | day == "Saturday") %>%
  mutate( weekday_end = "weekend") %>%
  select(week, day_id, day, weekday_end, everything())

weekday = 
  accel %>%
  filter(day == "Monday" | day == "Tuesday" | day == "Wednesday" | day == "Thursday" | day == "Friday") %>%
  mutate( weekday_end = "weekday") %>%
  select(week, day_id, day, weekday_end, everything())

accel =
  merge(weekend, weekday, all = TRUE) %>%
  arrange(week, day_id) %>%
    mutate_if(is.double, as.integer)
```

For this resulting dataset, there are `r nrow(accel)` observations and `r ncol(accel)` variables. It also includes the week, day ID, weekend or weekday information.

```{r echo=FALSE}
accel$sum = rowSums(accel[, 5:1440])
accel = select(accel, week, day_id, day, weekday_end, sum, everything())
table = select(accel, week, day_id, day, weekday_end, sum) 
knitr::kable(table)

#Are any trends apparent?
```

This table in not clear to see the trend. However, there are a few number greater than 10000 and most of them come from Thursday. It's relatively low at the first few weeks and than increase and finally go down.
 
```{r echo=FALSE}
table %>%
  #group_by(week) %>%
  select(week, day, sum) %>%
  ggplot(aes(x = week, y = sum, color = day)) +
  geom_point(size = 4, alpha = .5) + geom_line() +
  labs(
      title = "24-hour activity time courses",
      x = "Week",
      y = "Sum of a day"
    ) +
  viridis::scale_color_viridis(
    name ="Location",
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5))
  
```


Based on this graph, we can see the total number in Thursday is relatively high and fluctuate dramaticly. For Saturday, the value first wen up and than went down.
Most of them declined comparing with the 1st week.
